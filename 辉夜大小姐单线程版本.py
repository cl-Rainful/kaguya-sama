#!/usr/bin/python#description      :辉夜大小姐Spider#author         :Rainful#date          :20200628#version        :0.1#python_version     :3.8.2#==============================================================================import requestsfrom lxml import etreeimport osclass kaguyaComicSpider(object):    def __init__(self):        self.startNum = 201944768        #   设定最开始的数字        self.startUrl = "https://mhimg.eshanyao.com/ManHuaKu/h/huiyedaxiaojiexiangrangwogaobaitiancaimendelianait/"        self.charpet = 1        #   设定最开始章节        self.headers = {            "user-agent": "Mozilla / 5.0(Windows NT 10.0; Win64; x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome/81.0.4044.129 Safari / 537.36"        }    def parseUrl(self, url):        #   解析网页        response = requests.get(url=url, headers=self.headers)        return response.content.decode()    def etreeHtml(self, html):        html = etree.HTML(html)        return html    def infoSet(self):        #   读取已经爬取的章节        try:            with open("./charpetSet.txt", "r") as f:                info = f.readlines()                self.charpet = int(info[0].replace("\n", ""))                self.startNum = int(info[1])        except:            print("未找到charpetSet文件")            self.charpet = 1            self.startNum = 201944768    def saveInfo(self, charpet):        #   保存已经爬取的章节        with open("./charpetSet.txt", "w+") as f:            f.write(str(self.charpet) + '\n')            f.write(str(self.startNum))    def run(self):        self.infoSet()        print(self.charpet)        print(self.startNum)        while True:            resultUrl = self.startUrl + str(self.charpet) + "/" + str(self.startNum) + ".jpg"            #   拼接图片网址            if not os.path.exists('./漫画/{}'.format(self.charpet)):                os.makedirs('./漫画/{}'.format(self.charpet))            response = requests.get(url=resultUrl, headers=self.headers)            if response.status_code == 200:                #  检查图片网址是否可以访问，可以访问就下载，不可以访问就将章节+1，章节+1的网页可以访问就进入下一章，                # 不可以访问就说明Num是断号的，那么访问Num+1，不可以访问就循环上面的步骤，直到找到可以访问的图片                with open('./漫画/{}/{}.jpg'.format(self.charpet, self.startNum), 'ab') as f:                    f.write(response.content)                    print(self.startNum, 'get')                self.startNum += 1            else:                charpetPlusUrl = self.startUrl + str(self.charpet + 1) + "/" + str(self.startNum) + ".jpg"                response = requests.get(url=charpetPlusUrl, headers=self.headers)                if response.status_code == 200:                    self.charpet += 1                else:                    self.startNum += 1                print(charpetPlusUrl)            print(resultUrl)            if self.charpet > 183:                # 后续可以增加一个爬最大章节的，先就这么用着吧                break            self.saveInfo(self.charpet)if __name__ == '__main__':    spider = kaguyaComicSpider()    spider.run()